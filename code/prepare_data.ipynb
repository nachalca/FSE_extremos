{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to prepare the data to be used in future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                       \n",
    "import xarray as xr                      \n",
    "import pandas as pd     \n",
    "import cftime           \n",
    "import zipfile       \n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis_nc = xr.open_dataset('../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis/Datos_t2m_horario_2000a2010_uy.nc')\n",
    "reanalysis_nc_2 = xr.open_dataset('../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis/Datos_t2m_horario_2011a2021_uy.nc')\n",
    "\n",
    "def transform_reanalysis(data_nc):\n",
    "    reanalysis = pd.DataFrame()\n",
    "    reanalysis['time'] = pd.to_datetime(data_nc.time)\n",
    "    reanalysis['tas'] = data_nc.t2m.mean(dim=[\"latitude\",\"longitude\"]) \n",
    "    reanalysis = reanalysis.loc[reanalysis[\"time\"] < datetime.fromisoformat('2015-01-01T00:00:00')]\n",
    "    reanalysis['date'] = reanalysis['time'].dt.date\n",
    "    reanalysis['day'] = reanalysis['time'].dt.day\n",
    "    reanalysis['month'] = reanalysis['time'].dt.month\n",
    "  \n",
    "    ## TODO: Max and min has their own model, but for now we will calculate them here. But we should load them from their model.\n",
    "    reanalysis_by_day = reanalysis[[\"date\", \"tas\"]].groupby('date') \\\n",
    "                                                    .agg({'tas': ['mean', 'min', 'max']})\n",
    "    reanalysis_by_day.columns = ['_'.join(col).strip() for col in reanalysis_by_day.columns.values]\n",
    "    reanalysis_by_day = reanalysis_by_day.rename(columns={'tas_mean': 'tas_daily', 'tas_min': 'tas_min_daily', 'tas_max': 'tas_max_daily'})\n",
    "    ########################################################\n",
    "  \n",
    "    reanalysis = reanalysis.join(reanalysis_by_day, on=\"date\") \\\n",
    "                            .drop('date', axis=1)\n",
    "    return reanalysis\n",
    "\n",
    "reanalysis_tas = pd.concat([transform_reanalysis(reanalysis_nc), transform_reanalysis(reanalysis_nc_2)])\n",
    "reanalysis_tas = reanalysis_tas[((reanalysis_tas['day'] != 29) | (reanalysis_tas['month'] != 2))] #Delete all Feb 29th \n",
    "reanalysis_tas = reanalysis_tas[['time', 'tas_daily','tas_min_daily','tas_max_daily','tas']] #select the variables we want to keep\n",
    "reanalysis_tas[['time', 'tas_daily','tas']] .to_csv('reanalysis_tas_training_data.csv',  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CESM2_WACCM_nc = xr.open_dataset('../data/tas_day_CESM2-WACCM_historical_r1i1p1f1_gn_20000101-20141231_v20190227.nc')\n",
    "CESM2_WACCM = pd.DataFrame()\n",
    "CESM2_WACCM['time'] = CESM2_WACCM_nc.time\n",
    "CESM2_WACCM['CESM2_WACCM'] = CESM2_WACCM_nc.tas.mean(dim=[\"lat\",\"lon\"])\n",
    "CESM2_WACCM['time'] = CESM2_WACCM['time'].apply(lambda x: datetime.fromisoformat(x.isoformat()))\n",
    "\n",
    "CESM2_WACCM_by_hour = []\n",
    "\n",
    "for row in CESM2_WACCM.values:\n",
    "    for h in range(0,24):\n",
    "        new_row = pd.Series({\n",
    "            'time': row[0] + timedelta(hours=h),\n",
    "            'tas': 0,\n",
    "            'tas_daily': row[1]\n",
    "        })\n",
    "        CESM2_WACCM_by_hour.append(new_row)\n",
    "\n",
    "CESM2_WACCM = pd.DataFrame(CESM2_WACCM_by_hour)\n",
    "CESM2_WACCM.to_csv('cmip6_tas_to_downscale_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I've assumed that sfcwind = sqrt(u^2 + v^2). Corroboration is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis_wind_U_nc = xr.open_dataset('../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis/Datos_U10m_horario_2000a2010_uy.nc')\n",
    "reanalysis_wind_V_nc = xr.open_dataset('../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis/Datos_V10m_horario_2000a2010_uy.nc')\n",
    "reanalysis_wind_U_2_nc = xr.open_dataset('../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis/Datos_U10m_horario_2011a2021_uy.nc')\n",
    "reanalysis_wind_V_2_nc = xr.open_dataset('../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis/Datos_V10m_horario_2011a2021_uy.nc')\n",
    "\n",
    "\n",
    "def transform_reanalysis_wind(U_nc, V_nc):\n",
    "    reanalysis = pd.DataFrame()\n",
    "    reanalysis['time'] = pd.to_datetime(U_nc.time)\n",
    "    reanalysis['sfcWind'] = np.sqrt(np.square(U_nc.u10) + np.square(V_nc.v10)) \\\n",
    "                               .mean(dim=[\"latitude\",\"longitude\"]) \n",
    "    reanalysis = reanalysis.loc[reanalysis[\"time\"] < datetime.fromisoformat('2015-01-01T00:00:00')]\n",
    "    reanalysis['date'] = reanalysis['time'].dt.date\n",
    "    reanalysis['day'] = reanalysis['time'].dt.day\n",
    "    reanalysis['month'] = reanalysis['time'].dt.month\n",
    "    reanalysis_by_day = reanalysis[[\"date\", \"sfcWind\"]].groupby('date') \\\n",
    "                                                   .mean() \\\n",
    "                                                   .rename(columns={\"sfcWind\":\"sfcWind_daily\"})\n",
    "    reanalysis = reanalysis.join(reanalysis_by_day, on=\"date\") \\\n",
    "                            .drop('date', axis=1)\n",
    "    return reanalysis\n",
    "\n",
    "reanalysis_wind = pd.concat([transform_reanalysis_wind(reanalysis_wind_U_nc, reanalysis_wind_V_nc), \n",
    "                             transform_reanalysis_wind(reanalysis_wind_U_2_nc, reanalysis_wind_V_2_nc)])\n",
    "\n",
    "reanalysis_wind = reanalysis_wind[((reanalysis_wind['day'] != 29) | (reanalysis_wind['month'] != 2))] #Delete all Feb 29th \n",
    "reanalysis_wind = reanalysis_wind[['time', 'sfcWind_daily', 'sfcWind']] #select the variables we want to keep\n",
    "reanalysis_wind.to_csv('reanalysis_sfcWind_training_data.csv',  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CESM2_WACCM_wind_nc = xr.open_dataset('../data/sfcWind_day_CESM2-WACCM_historical_r1i1p1f1_gn_20000101-20141231_v20190227.nc')\n",
    "CESM2_WACCM_wind = pd.DataFrame()\n",
    "CESM2_WACCM_wind['time'] = CESM2_WACCM_wind_nc.time\n",
    "CESM2_WACCM_wind['CESM2_WACCM'] = CESM2_WACCM_wind_nc.sfcWind.mean(dim=[\"lat\",\"lon\"])\n",
    "CESM2_WACCM_wind['time'] = CESM2_WACCM_wind['time'].apply(lambda x: datetime.fromisoformat(x.isoformat()))\n",
    "\n",
    "CESM2_WACCM_wind_by_hour = []\n",
    "\n",
    "for row in CESM2_WACCM_wind.values:\n",
    "    for h in range(0,24):\n",
    "        new_row = pd.Series({\n",
    "            'time': row[0] + timedelta(hours=h),\n",
    "            'sfcWind': 0,\n",
    "            'sfcWind_daily': row[1]\n",
    "        })\n",
    "        CESM2_WACCM_wind_by_hour.append(new_row)\n",
    "\n",
    "CESM2_WACCM_wind = pd.DataFrame(CESM2_WACCM_wind_by_hour)\n",
    "CESM2_WACCM_wind.to_csv('cmip6_sfcWind_to_downscale_data.csv', index=False)\n",
    "#CESM2_WACCM_wind.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECIPTITATION_FOLDER = \"../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis-precipitation/\"\n",
    "\n",
    "def transform_reanalysis(data_nc):\n",
    "    reanalysis_precipitation = pd.DataFrame()\n",
    "    try:\n",
    "        reanalysis_precipitation['time'] = pd.to_datetime(data_nc.time)\n",
    "        reanalysis_precipitation['pr'] = data_nc.tp.mean(dim=[\"latitude\",\"longitude\"])*1000\n",
    "    except:\n",
    "        return reanalysis_precipitation\n",
    "    reanalysis_precipitation['date'] = reanalysis_precipitation['time'].dt.date\n",
    "    reanalysis_precipitation['day'] = reanalysis_precipitation['time'].dt.day\n",
    "    reanalysis_precipitation['month'] = reanalysis_precipitation['time'].dt.month\n",
    "    reanalysis_precipitation_by_day = reanalysis_precipitation[[\"date\", \"pr\"]].groupby('date') \\\n",
    "                                                   .mean() \\\n",
    "                                                   .rename(columns={\"pr\":\"pr_daily\"})\n",
    "    reanalysis_precipitation = reanalysis_precipitation.join(reanalysis_precipitation_by_day, on=\"date\") \\\n",
    "                            .drop('date', axis=1)\n",
    "    return reanalysis_precipitation\n",
    "\n",
    "reanalysis_precipitation = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(PRECIPTITATION_FOLDER):\n",
    "\n",
    "    with zipfile.ZipFile(PRECIPTITATION_FOLDER + file,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(PRECIPTITATION_FOLDER + \"data\")\n",
    "    \n",
    "    precipitation_nc = xr.open_dataset(PRECIPTITATION_FOLDER + \"data/data.nc\")\n",
    "    df = transform_reanalysis(precipitation_nc)\n",
    "    reanalysis_precipitation = pd.concat([reanalysis_precipitation, df])\n",
    "    os.remove(PRECIPTITATION_FOLDER + \"data/data.nc\")\n",
    "    os.removedirs(PRECIPTITATION_FOLDER + \"data\")\n",
    "\n",
    "reanalysis_precipitation.sort_values(by='time', inplace=True)\n",
    "reanalysis_precipitation.reset_index(drop=True, inplace=True)\n",
    "reanalysis_precipitation = reanalysis_precipitation.loc[reanalysis_precipitation[\"time\"] < datetime.fromisoformat('2015-01-01T00:00:00')]\n",
    "reanalysis_precipitation = reanalysis_precipitation[((reanalysis_precipitation['day'] != 29) | (reanalysis_precipitation['month'] != 2))] #Delete all Feb 29th \n",
    "reanalysis_precipitation = reanalysis_precipitation[['time', 'pr_daily', 'pr']] #select the variables we want to keep\n",
    "reanalysis_precipitation.to_csv('reanalysis_precipitation_training_data.csv',  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CESM2_WACCM_precipitation_nc = xr.open_dataset('../data/pr_day_CESM2-WACCM_historical_r1i1p1f1_gn_20000101-20141231_v20190415.nc')\n",
    "CESM2_WACCM_precipitation = pd.DataFrame()\n",
    "CESM2_WACCM_precipitation['time'] = CESM2_WACCM_precipitation_nc.time\n",
    "CESM2_WACCM_precipitation['CESM2_WACCM'] = CESM2_WACCM_precipitation_nc.pr.mean(dim=[\"lat\",\"lon\"])*3600\n",
    "CESM2_WACCM_precipitation['time'] = CESM2_WACCM_precipitation['time'].apply(lambda x: datetime.fromisoformat(x.isoformat()))\n",
    "\n",
    "CESM2_WACCM_precipitation_by_hour = []\n",
    "\n",
    "for row in CESM2_WACCM_precipitation.values:\n",
    "    for h in range(0,24):\n",
    "        new_row = pd.Series({\n",
    "            'time': row[0] + timedelta(hours=h),\n",
    "            'pr': 0,\n",
    "            'pr_daily': row[1]\n",
    "        })\n",
    "        CESM2_WACCM_precipitation_by_hour.append(new_row)\n",
    "\n",
    "CESM2_WACCM_precipitation = pd.DataFrame(CESM2_WACCM_precipitation_by_hour)\n",
    "CESM2_WACCM_precipitation.to_csv('cmip6_precipitation_to_downscale_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface Pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURFACE_PRESSURE_FOLDER = \"../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis-surface_pressure/\"\n",
    "\n",
    "def transform_reanalysis_surface_pressure(data_nc):\n",
    "    reanalysis_surface_pressure = pd.DataFrame()\n",
    "    try:\n",
    "        reanalysis_surface_pressure['time'] = pd.to_datetime(data_nc.time)\n",
    "        reanalysis_surface_pressure['sp'] = data_nc.sp.mean(dim=[\"latitude\",\"longitude\"])\n",
    "    except:\n",
    "        return reanalysis_surface_pressure\n",
    "    reanalysis_surface_pressure['date'] = reanalysis_surface_pressure['time'].dt.date\n",
    "    reanalysis_surface_pressure['day'] = reanalysis_surface_pressure['time'].dt.day\n",
    "    reanalysis_surface_pressure['month'] = reanalysis_surface_pressure['time'].dt.month\n",
    "    reanalysis_surface_pressure_by_day = reanalysis_surface_pressure[[\"date\", \"sp\"]].groupby('date') \\\n",
    "                                                   .mean() \\\n",
    "                                                   .rename(columns={\"sp\":\"sp_daily\"})\n",
    "    reanalysis_surface_pressure = reanalysis_surface_pressure.join(reanalysis_surface_pressure_by_day, on=\"date\") \\\n",
    "                            .drop('date', axis=1)\n",
    "    return reanalysis_surface_pressure\n",
    "\n",
    "reanalysis_surface_pressure = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(SURFACE_PRESSURE_FOLDER):\n",
    "\n",
    "    with zipfile.ZipFile(SURFACE_PRESSURE_FOLDER + file,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(SURFACE_PRESSURE_FOLDER + \"data\")\n",
    "    \n",
    "    surface_pressure_nc = xr.open_dataset(SURFACE_PRESSURE_FOLDER + \"data/data.nc\")\n",
    "    df = transform_reanalysis_surface_pressure(surface_pressure_nc)\n",
    "    reanalysis_surface_pressure = pd.concat([reanalysis_surface_pressure, df])\n",
    "    os.remove(SURFACE_PRESSURE_FOLDER + \"data/data.nc\")\n",
    "    os.removedirs(SURFACE_PRESSURE_FOLDER + \"data\")\n",
    "\n",
    "reanalysis_surface_pressure.sort_values(by='time', inplace=True)\n",
    "reanalysis_surface_pressure.reset_index(drop=True, inplace=True)\n",
    "reanalysis_surface_pressure = reanalysis_surface_pressure[((reanalysis_surface_pressure['day'] != 29) | (reanalysis_surface_pressure['month'] != 2))] #Delete all Feb 29th \n",
    "reanalysis_surface_pressure = reanalysis_surface_pressure[['time', 'sp_daily', 'sp']] #select the variables we want to keep\n",
    "reanalysis_surface_pressure = reanalysis_surface_pressure.loc[reanalysis_surface_pressure[\"time\"] < datetime.fromisoformat('2015-01-01T00:00:00')]\n",
    "reanalysis_surface_pressure.to_csv('reanalysis_surface_pressure_training_data.csv',  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Cloud Percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis_nc = xr.open_dataset('../data/Archivos_reanalisisERA5_realesINUMET/Reanalisis - total_cloud_percentage/total_cloud_percentage.nc')\n",
    "\n",
    "reanalysis_tot_cloud_pctg = transform_reanalysis_tot_cloud_pctg(reanalysis_nc)\n",
    "#reanalysis_tot_cloud_pctg = reanalysis_tot_cloud_pctg[['time', 'tas_daily','tas_min_daily','tas_max_daily','tas']] #select the variables we want to keep\n",
    "reanalysis_tot_cloud_pctg[['time', 'tas_daily','tas']].to_csv('reanalysis_tas_training_data.csv',  index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tas_daily</th>\n",
       "      <th>tas_min_daily</th>\n",
       "      <th>tas_max_daily</th>\n",
       "      <th>tas</th>\n",
       "      <th>sfcWind_daily</th>\n",
       "      <th>sfcWind</th>\n",
       "      <th>pr_daily</th>\n",
       "      <th>pr</th>\n",
       "      <th>sp_daily</th>\n",
       "      <th>sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>295.357391</td>\n",
       "      <td>289.210754</td>\n",
       "      <td>301.250488</td>\n",
       "      <td>295.123260</td>\n",
       "      <td>4.347946</td>\n",
       "      <td>4.526224</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>0.128054</td>\n",
       "      <td>100092.25</td>\n",
       "      <td>100063.445312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-01 01:00:00</td>\n",
       "      <td>295.357391</td>\n",
       "      <td>289.210754</td>\n",
       "      <td>301.250488</td>\n",
       "      <td>294.918762</td>\n",
       "      <td>4.347946</td>\n",
       "      <td>4.780273</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>100092.25</td>\n",
       "      <td>100144.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-01 02:00:00</td>\n",
       "      <td>295.357391</td>\n",
       "      <td>289.210754</td>\n",
       "      <td>301.250488</td>\n",
       "      <td>294.258972</td>\n",
       "      <td>4.347946</td>\n",
       "      <td>4.837720</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>100092.25</td>\n",
       "      <td>100184.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-01 03:00:00</td>\n",
       "      <td>295.357391</td>\n",
       "      <td>289.210754</td>\n",
       "      <td>301.250488</td>\n",
       "      <td>291.075500</td>\n",
       "      <td>4.347946</td>\n",
       "      <td>4.704229</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>100092.25</td>\n",
       "      <td>100177.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-01 04:00:00</td>\n",
       "      <td>295.357391</td>\n",
       "      <td>289.210754</td>\n",
       "      <td>301.250488</td>\n",
       "      <td>290.787567</td>\n",
       "      <td>4.347946</td>\n",
       "      <td>4.446250</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>0.006419</td>\n",
       "      <td>100092.25</td>\n",
       "      <td>100174.578125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time   tas_daily  tas_min_daily  tas_max_daily         tas  \\\n",
       "0 2000-01-01 00:00:00  295.357391     289.210754     301.250488  295.123260   \n",
       "1 2000-01-01 01:00:00  295.357391     289.210754     301.250488  294.918762   \n",
       "2 2000-01-01 02:00:00  295.357391     289.210754     301.250488  294.258972   \n",
       "3 2000-01-01 03:00:00  295.357391     289.210754     301.250488  291.075500   \n",
       "4 2000-01-01 04:00:00  295.357391     289.210754     301.250488  290.787567   \n",
       "\n",
       "   sfcWind_daily   sfcWind  pr_daily        pr   sp_daily             sp  \n",
       "0       4.347946  4.526224  0.018645  0.128054  100092.25  100063.445312  \n",
       "1       4.347946  4.780273  0.018645  0.001557  100092.25  100144.429688  \n",
       "2       4.347946  4.837720  0.018645  0.004051  100092.25  100184.609375  \n",
       "3       4.347946  4.704229  0.018645  0.005007  100092.25  100177.500000  \n",
       "4       4.347946  4.446250  0.018645  0.006419  100092.25  100174.578125  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all the reanalysis data\n",
    "reanalysis = reanalysis_tas.merge(reanalysis_wind, on='time', how='inner') \\\n",
    "                            .merge(reanalysis_precipitation, on='time', how='inner') \\\n",
    "                            .merge(reanalysis_surface_pressure, on='time', how='inner')\n",
    "reanalysis.to_csv('reanalysis.csv', index=False)\n",
    "reanalysis.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
