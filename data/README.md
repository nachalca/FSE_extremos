## Following is a brief description of the subfolders contained within this folder

|Folder|Description|
|------|-----------|
|`Archivos_reanalisisERA5_realesINUMET`| Datasets that were provided by Santiago. Here we have real observations for the years 2020 and 2021. We also have CMIP data from 2000-2021. The region of the CMIP data is slightly smaller than the one we downloaded. We are not using either of these datasets.|
|`cmip`|This folder contains the `historical` and `projections` subfolder. The `historical` data (2000-2014) was downloaded by Anastassia, Mohamed, and Amine; currently (we are not using these datasets). In the `projections` folder contains a subfolder for each CMIP model. Each one of these subfolders contains, for each experiment, a subfolder in which if the script was able to download all the files, there is a `CSV` file with the name of the experiment. Otherwise, there is a file with the name `UNCOMPLETED`, which contains the reason why the script failed. The data was downloaded with the cmip_data.py script.|
|`external_data`|Contains two dataset: `sun.csv` which provides hourly data on the sun's elevation and azimuth for the period 1980-2100. `daylight.csv` which provides daily daylight amount (in seconds). For these two files we took the lat -32.5 and the lon -56 (the center of the region) and an elevation of 0 as the observer's position. These datasets were generated with the script `sun_data.py` using the pbliv package.|
|`reanalysis`|We have a `reanalysis.csv` with the hourly values of all the variable of interest from the period 1980-2023. Also for each variable we have a corresponding subfolder (note that  we are using the reanalysis names of the variables). Each subfolder contains a `csv` which is the result of the transformation of the original `nc` files downloaded from the `cds`. The process of downloading and transforming the dataset was made by the script `reanalyisis_data.py`.|
|`testing`|Contains the datasets that were used to test the models and generate the `model_evaluation` reports. These datasets were built with the `training_data_generator.py` using the `reanalysis.csv` as the source. The train/test split was made in the models file but I will move the split to the training_data_generator for more consistency.|
|`to_be_downscaled`|Contains datasets to be downscaled. There are subfolders for each variable (that we want to downscale), in which, for each CMIP model-experiment, there is a correspondant dataset (a CSV file). We use these files for build the `downscaling_evaluation` report.|
|`training`|Contains the datasets that were used to train the models. We have a distinct dataset for each variable that we want to downscale. These datasets were built with the `training_data_generator.py` using the `reanalysis.csv` as the source. The train/test split was made in the models file but I will move the split to the `training_data_generator.py` for more consistency.|
|`validation`|These are the dataset used to build the `validation_report`. For each variable we have a `CSV` file with one column for the reanalysis data and additional columns for each CMIP model-experiment combination. We are using the time scale of the `CMIP` data for both reanalysis and `CMIP`. All the transformations were made with the script `validation_data.py`.|